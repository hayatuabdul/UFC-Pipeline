# UFC-Pipeline
A Data Engineering project that extracts UFC data from a REST API and performs ETL to load in Snowflake. Automation is carried out by Apache Airflow hosted in a Docker container.
The Docker files were removed due to incompatabilities with Git.

Project Description: Building an Automated Data Pipeline for UFC Fights and Fighter Information Using Python, Snowflake, and Apache Airflow

Overview:
This data engineering project focuses on creating a robust pipeline to manage UFC event and fighter data. Utilizing a REST API, the project extracts information about fights, events, and fighters, processes it using a Python script, and loads it into a Snowflake data warehouse. The primary goal is to centralize UFC-related data for SQL analysis, providing detailed insights into fighters, events, winners, rounds, time, methods of winning, and fight ratings.

Project Goals:
1. Extract comprehensive UFC data from a REST API.
2. Transform the extracted data using a Python script.
3. Load the transformed data into a Snowflake data warehouse for SQL analysis.
4. Implement SQL cursor and Snowflake connector in Python to re-extract data from the Snowflake table.
5. Automate the entire process using Apache Airflow to schedule weekly extractions and updates.

Technologies Used:
- Python
- Snowflake data warehouse
- Apache Airflow
- Docker

Key Features:
1. Data Extraction:
   - Utilized a REST API to gather information about UFC fights, events, and fighters.
   - Ensured a one-time extraction to prevent overusing the API and stored the data in Snowflake.

2. Snowflake Data Extraction:
   - Implemented SQL cursor and Snowflake connector in Python to efficiently re-extract data from the Snowflake table generated by the initial script.

3. Automation with Apache Airflow:
   - Employed Apache Airflow to automate the entire data pipeline.
   - Hosted Apache Airflow in a Docker container.
   - Scheduled weekly DAGs (Directed Acyclic Graphs) to extract fresh data from the API, transforming it, and updating the Snowflake database.

Project Workflow:
1. Data Extraction: Extracted UFC data from the REST API.
2. Transformation: Utilized a Python script to transform the data, ensuring it meets the desired format.
3. Loading: Loaded the processed data into a Snowflake data warehouse for efficient storage and SQL querying.
4. Snowflake Extraction: Used SQL cursor and Snowflake connector to re-extract data from the Snowflake table.
5. Automation: Orchestrated the entire process using Apache Airflow, ensuring weekly updates and maintaining data accuracy.

Skills Demonstrated:
- Proficiency in Python for data extraction, transformation, and loading (ETL) processes.
- Utilization of Snowflake data warehouse for efficient data storage and retrieval.
- Implementation of SQL cursor and Snowflake connector for seamless data extraction within Python scripts.
- Docker containerization for hosting Apache Airflow.
- Creation and scheduling of DAGs in Apache Airflow for automated weekly updates.

This project serves as a testament to my expertise in designing and managing end-to-end data pipelines. By seamlessly integrating various technologies, I've demonstrated the ability to handle complex data engineering tasks, ensuring the availability of up-to-date UFC-related data for analytical purposes.
